---
title: "Charismatic Leadership Tactics Assessment"
categories:
  - Technical 
toc: true
toc_sticky: true
toc_title: "Contents"
---

> Predicting leadership qualities using machine learning (and some deep learning) with 77 data points

# "He could sell ice to an eskimo"   

Why do some people instantly capture attention while others who are just as capable fade into the background? Charisma shapes whose voices rise and whose ideas get heard. Charisma is not the same as intelligence. Some people naturally project confidence, warmth and energy even when their message is simple. Others have great ideas but struggle to communicate them in a way that resonates. These differences shape everyday situations from presentations in class to job interviews and leadership roles. Understanding what makes communication engaging can help train people to become effective communicators. 

Charisma includes auditory cues (e.g., tone), linguistic strategies (e.g., anecdotes), and visual signals (e.g., hand gestures). The automated assessment of such traits represents an intersection of Computer Vision, Natural Language Processing, and Psychology. Charisma assessment is inherently challenging because it relies on the integration of heterogeneous cues: no single modality can capture its full complexity. For instance, the perception of "enthusiastic" often emerges from the interplay of an animated tone of voice, specific rhetorical devices, and open body language, rather than any single factor in isolation.

In this post I explain my second group research project @ Maastricht University where we built a system to predict charismatic traits given a video input. 

# Methodology 

Our data comes from a psychological study of university students. The researchers recorded participants speaking impromptu on a given prompt before and after receiving leadership training, and asked people to rate each video across multiple dimensions. These videos are the inputs we use to predict the survey ratings. 

Our final solution involved two stages. First, we extract interpretable features from the three modalities [vision, speech, and text] using different pre-trained deep learning methods. Second, we train Decision Trees on these features to predict the ratings. This division allows for independent development across individual modalities, making combining different results trivial. A visual representation is shown below. 

![](/assets/img/rp2/Final%20sol.png)

Through our work we hoped to answer four research questions: 

- $\textbf{RQ1.}$ Which data modality, by itself, holds the strongest predictive power?
- $\textbf{RQ2.}$ What metrics are relevant for comparing detection across different modalities, and how can they be combined in a meaningful manner?
- $\textbf{RQ3.}$ What multi-modal architectures best balance predictive accuracy and explanatory power for charismatic trait inference from long-form video, audio, and text?
- $\textbf{RQ4.}$ How can agentic frameworks be implemented to provide actionable, intepretable and user specific predictions?

We needed X main components to make this all work which are visualized below along with my contribution. 

![](/assets/img/rp2/whodidwhat.jpg)

## Pre-processing and label generation 

I start by explaining the all important labels for the task and how I calculated them for each participant. 

![](/assets/img/rp2/reviews.png)

The structure of the survey reviews dataset 

Each row here is a single person who rates four different videos, two pre-videos and two post-videos (identified by their unique code). They also enter some meta-data like the type of device they are using and their age and gender. They rate the participants across 17 different charisma dimensions, each score being an integer in a certain range. However, during preprocessing, I found an error in the study, only the pre-videos were shown to the survey respondents. As a result, the usable data contains one video per student, reducing the dataset to 77 unique videos. For this reason we decided not to use deep learning methods directly to make predictions, the models wouldn't train very well and the high-dimensional embeddings would be undecipherable. 

The original survey had 17 labels, to reduce reliance on the most subjective ratings, we excluded five labels. I then cleaned the survey data by (i) removing incomplete responses, (ii) standardizing column names, and (iii) extracting numeric ratings from mixed-format entries ('1notatall' $\to$ 1). The label are a mix of multi-modal characteristics (warmth) and uni-modal characteristics (appropriate facial expressions), the full list of the 11 labels is in the Appendix. For each participant, I collect all their ratings from different respondents and finally store their average score per label. Along with these, I also store each participants' gender (don't worry we don't use this to make predictions). All this done, we end up a neat little file with 77 rows and 13 columns. 

A few things to note: 

- Each video is not shown an equal number of times, and averaging the scores masks this detail. Some videos were clearly more popular than others, any final considerations should take this into account.  
- There are almost equal male and females in the study, both in the participants and the respondents themselves. This means there is no need to re-sample the data points for creating a uniform sample.  
- Both label ratings and survey responders follow a unimodal distribution without any heavy skews. This means that most students perform similarly well and the critics are not easily impressed or overtly harsh, a good sign. (Plots are in the Appendix) 

## Vision features

## Speech features

## Text features

# Experiments and results 

Phew, that was a lot of words. Before we move on to the experimental set up and the findings, here's a quick overview of the three features and some interesting properties. 

![](/assets/img/rp2/Overview.jpg)

The vision features reply heavily on manual thresholds (gaze, body motion, emotion) informed by three people watching a limited number of different videos. Luckily most counts are non-zero, which means the decision trees can work reliably. 

The features drawn from speech are purely statistical, they do little to almost no post processing and are as close to the raw input signal as possible. Luckily, they don't suffer from as much sparsity as the text modality. Unfortunately the minimum duration of pauses (5 sec) is set manually based on limited videos. This one issue stops it from being the almost ideal set of features!  

Text on the other hand is hit hard by sparsity, which is probably the major reason for it's poor performance. It should be noted that the features prescribed to us were maybe overkill given that the participants in question were amateurs in training and not seasoned orators. It is still a derived feature, although this is not a concern given the meteoric rise in LLMs' language processing. This was the one set of features that was free of any manual thresholds, and while it still relies on labels, no manual tweaks are needed, making it that much better. 

The main results aside we performed a quick sanity check to see if an LLM predicting the text labels from transcripts could lead to better result than our long-winded approach. Luckily for us it performed much worse, and paradoxically made us feel better about our not so great text features. 

# What's next? 

I like to think our work mainly informs the next group what not to do! I quit joking around and articulate directions of future work below: 

- **Reliable ground truths for features:** we came up with the explainable features ourselves, checking each model's performance against manual performance on the same input. For quality feature generation I would start by finding some reliable and repeatable means of ground truth, enhancing the quality of features and hopefully the final predictions too.  

- **Reduce or remove magic numbers:** all three systems in vision and the logic in speech relied on thresholds estimated by various members of the team on independent videos. Apart from this systemic problem, this solution does not scale and is heavily biased on the samples selected. A deeper analysis of the data can be performed to guide machine learning approaches apart from any insights from literature. 

- **Multi-dimensional correlation:** the correlation done in this instance was binary, only a pair of (label (or) feature) were compared. We know from psychology that Charisma is a multi-modal phenomena, so visualizing the 'Warmth' label as a linear combination of uni-modal features might be a good starting point. A more thorough study of this topic is sure to produce non-trivial insights. Care must be made that the combinations scale exponentially and the signal to noise ratio is extremely small.    

- **Temporal features:** we had hoped to build a temporal database of features across each modality. This could then be fed into an LLM for mildly sophisticated reasoning. One could then expect responses like 'Your rating for confidence is low, you seem to smile only near the end unlike your peers with high scores who smile at [x,y,z].' A basic setup was created by storing the frames that lead to a certain decision but was not fully developed and integrated. Creating a data representation with temporal aspects can lead not only to higher predictability but also interpretability.   

- **Feature engineering:** although it was easy to spo that we needed new features on the text modality sue to their sparsity, non-zero counts are not a seal of quality. The next team needs to critically evaluate the set of features we've proposed across all modalities and come up with something more suitable for the dataset in question while being grounded in literature / practice. 

# Appendix 

Below you will find a long list of all the labels and their meanings, along with all the plots did not fit in the main post. 

**Labels:** 

_Multi-modal_ 

- L1: Charismatic 
- L2: Likable 
- L3: Warm 
- L4: Enthusiastic
- L5: Inspiring

_Text_

- L6: Has a clear understanding of where we are going 
- L7: Paints an interesting picture of where we are going as a group 
- L8: Inspires others with his/her plans for the future 
- L9: Able to get others committed to his/her dream

_Vision_

- L10: Uses appropriate facial expressions

_Speech_

- L11: Uses an animated tone of voice

**Features:**

_Text_ 

- Meta: Uses a metaphor (count)
- Rhet:	Poses a rhetorical question (count)
- Stor:	Tells a story or anecdote (count)
- Cont:	Uses a contrast (count)
- List:	Uses a list (with 3 or more parts) (count)
- Mora:	Makes a moral appeal (count)
- Sent:	Expresses the sentiment of the collective (count)
- Ambi:	Sets an ambitious goal (count)
- Conf:	Communicates confidence (count)

_Vision_ 

- HM: Hand movements (count, number of frames)
- BN: Body sway (count, number of frames)
- DWN: Looking down (count, number of frames)
- EC: Eye contact (float, percentage of eye contact was held)
- POS: Positive emotions (count, number of frames)
- NEG: Negative emotions (count, number of frames)
- SML: Smiles (count, number of frames)

_Speech_ 

- DUR: Duration (seconds, entire video length)
- ST: Speech time (seconds, talking length)
- SILT: Silence time (seconds, silence length)
- NP: Number of pauses (count)
- MP: Mean duration of pauses (seconds)
- MXP: Maximum pause duration (seconds)
- MXS: Maximum speech duration (seconds)
- F0M: F0 Mean 
- F0STD: F0 Standard deviation 
- F0R: F0 Range 
- RMSE: RMS Energy 
- FC: Filler count (count)

**Graphs** 

![](/assets/img/rp2/ratings.png)

Distribution of participant scores 

![](/assets/img/rp2/raters.png)

Distribution of survey responders ratings habits 