---
title: "Charismatic Leadership Tactics Assessment"
categories:
  - Technical 
toc: true
toc_sticky: true
toc_title: "Contents"
---

> Predicting leadership qualities using machine learning (and some deep learning) with 77 data points

# He could sell ice to an eskimo   

Why do some people instantly capture attention while others who may be just as capable fade into the background? Charisma shapes whose voices rise and whose ideas get heard. Charisma is not the same as intelligence. Some people naturally project confidence, warmth and energy even when their message is simple. Others have great ideas but struggle to communicate them in a way that resonates. These differences shape everyday situations from presentations in class to job interviews and leadership roles. Understanding what makes communication engaging can help train people to become effective communicators. 

Charisma includes auditory cues (e.g., tone), linguistic strategies (e.g., anecdotes), and visual signals (e.g., hand gestures). The automated assessment of such traits represents an intersection of Computer Vision, Natural Language Processing, and Social Psychology. Charisma assessment is inherently challenging because it relies on the integration of heterogeneous cues: no single modality can capture its full complexity. For instance, the perception of "enthusiastic" often emerges from the interplay of an animated tone of voice, specific rhetorical devices, and open body language, rather than any single factor in isolation.

In this work, we present a multimodal framework designed to automatically detect and quantify these charismatic traits across video, audio, and text. Unlike "black-box" deep learning approaches that rely on opaque high-dimensional embeddings, our primary objective is explainability. We posit that decisions regarding social 3 traits must be transparent to be actionable. Therefore, our pipeline utilizes deep learning techniques solely to extract semantic, human-interpretable features (such as gaze direction, rhetorical device counts, and pitch statistics). These features are then aggregated to predict charisma scores using interpretable machine learning models. 

# Methodology 

Our data comes from a psychological study of university students. The researchers recorded participants speaking on a given prompt before and after receiving leadership training and asked people to rate each video across multiple dimensions.  

WHO DID WHAT  

## Pre-processing and label generation 

![](/assets/img/rp2/reviews.png)

The structure of the survey reviews dataset 


## Vision features

## Speech features

## Text features

## Experiments and results

Phew, that was a lot of words. Before we move on to the experimental set up and the findings, here's a quick overview of the three features and some interesting properties. 

![](/assets/img/rp2/Overview.jpg)

The vision features reply heavily on manual thresholds (gaze, body motion, emotion) informed by three people watching a limited number of different videos. Luckily most counts are non-zero, which means the decision trees can work reliably. 

The features drawn from speech are purely statistical, they do little to almost no post processing and are as close to the raw input signal as possible. Luckily, they don't suffer from as much sparsity as the text modality. Unfortunately the minimum duration of pauses (5 sec) is set manually based on limited videos. This one issue stops it from being the almost ideal set of features!  

Text on the other hand is hit hard by sparsity, which is probably the major reason for it's poor performance. It should be noted that the features prescribed to us were maybe overkill given that the participants in question were amateurs in training and not seasoned orators. It is still a derived feature, although this is not a concern given the meteoric rise in LLMs' language processing. This was the one set of features that was free of any manual thresholds, and while it still relies on labels, no manual tweaks are needed, making it that much better. 

The main results aside we performed a quick sanity check to see if an LLM predicting the text labels from transcripts could lead to better result than our long-winded approach. Luckily for us it performed much worse, and paradoxically made us feel better about our not so great text features. 

# What's next? 

I like to think our work mainly informs the next group what not to do! I quit joking around and articulate directions of future work below: 

- Reliable ground truths for features: we came up with the explainable features ourselves, checking each model's performance against manual performance on the same input. For quality feature generation I would start by finding some reliable and repeatable means of ground truth, enhancing the quality of features and hopefully the final predictions too.  

- Reduce or remove magic numbers: all three systems in vision and the logic in speech relied on thresholds estimated by various members of the team on independent videos. Apart from this systemic problem, this solution does not scale and is heavily biased on the samples selected. A deeper analysis of the data can be performed to guide machine learning approaches apart from any insights from literature. 

- Multi-dimensional correlation: the correlation done in this instance was binary, only a pair of (label (or) feature) were compared. We know from psychology that Charisma is a multi-modal phenomena, so visualizing the 'Warmth' label as a linear combination of uni-modal features might be a good starting point. A more thorough study of this topic is sure to produce non-trivial insights. Care must be made that the combinations scale exponentially and the signal to noise ratio is extremely small.    

- Temporal features: we had hoped to build a temporal database of features across each modality. This could then be fed into an LLM for mildly sophisticated reasoning. One could then expect responses like 'Your rating for confidence is low, you seem to smile only near the end unlike your peers with high scores who smile at [x,y,z].' A basic setup was created by storing the frames that lead to a certain decision but was not fully developed and integrated. Creating a data representation with temporal aspects can lead not only to higher predictability but also interpretability.   

- Feature engineering: although it was easy to spo that we needed new features on the text modality sue to their sparsity, non-zero counts are not a seal of quality. The next team needs to critically evaluate the set of features we've proposed across all modalities and come up with something more suitable for the dataset in question while being grounded in literature / practice. 

# Appendix 

Below you will find a long list of all the labels and their meanings, along with all the plots did not fit in the main post. 

**Labels:** 

_Multi-modal_ 

- L1: Charismatic 
- L2: Likable 
- L3: Warm 
- L4: Enthusiastic
- L5: Inspiring

_Text_

- L6: Has a clear understanding of where we are going 
- L7: Paints an interesting picture of where we are going as a group 
- L8: Inspires others with his/her plans for the future 
- L9: Able to get others committed to his/her dream

_Vision_

- L10: Uses appropriate facial expressions

_Speech_

- L11: Uses an animated tone of voice

**Features:**

_Text_ 

- Meta: Uses a metaphor (count)
- Rhet:	Poses a rhetorical question (count)
- Stor:	Tells a story or anecdote (count)
- Cont:	Uses a contrast (count)
- List:	Uses a list (with 3 or more parts) (count)
- Mora:	Makes a moral appeal (count)
- Sent:	Expresses the sentiment of the collective (count)
- Ambi:	Sets an ambitious goal (count)
- Conf:	Communicates confidence (count)

_Vision_ 

- HM: Hand movements (count, number of frames)
- BN: Body sway (count, number of frames)
- DWN: Looking down (count, number of frames)
- EC: Eye contact (float, percentage of eye contact was held)
- POS: Positive emotions (count, number of frames)
- NEG: Negative emotions (count, number of frames)
- SML: Smiles (count, number of frames)

_Speech_ 

- DUR: Duration (seconds, entire video length)
- ST: Speech time (seconds, talking length)
- SILT: Silence time (seconds, silence length)
- NP: Number of pauses (count)
- MP: Mean duration of pauses (seconds)
- MXP: Maximum pause duration (seconds)
- MXS: Maximum speech duration (seconds)
- F0M: F0 Mean 
- F0STD: F0 Standard deviation 
- F0R: F0 Range 
- RMSE: RMS Energy 
- FC: Filler count (count)

**Graphs** 

